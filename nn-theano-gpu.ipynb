{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed things up further Theano can [run your code on a GPU](http://deeplearning.net/software/theano/tutorial/using_gpu.html). Theano relies on Nvidia's CUDA platform, so yo need a [CUDA-enabled graphics card](http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-mac-os-x/#cuda-enabled-gpu). If you don't have one (for example, if you're on a Macbook with an integrated GPU) you could start up a [GPU-optimized Amazon EC2 instance](https://aws.amazon.com/ec2/instance-types/#gpu) and try things there. To use the GPU with Theano you must have the CUDA toolkit installed and [set a few environment variables](http://deeplearning.net/software/theano/install.html#gpu-linux).\n",
    "\n",
    "While any Theano code will run just fine on the GPU you will most likely need to spend some time optimizing it to get optimal performance. Running code that isn't optimized for the GPU may actually be slower than using the CPU due to data transfer to and from the GPU.\n",
    "\n",
    "Here are the some things you need to consider when you want to use a GPU:\n",
    "\n",
    "- The default floating point data type is `float64`, but in order to use the GPU you must use `float32`. There are several ways to do this in Theano. You can either use the [fulled typed tensor constructors](http://deeplearning.net/software/theano/library/tensor/basic.html#all-fully-typed-constructors) or set  `theano.config.floatX` to `float32` and use the [simple tensor constructors](http://deeplearning.net/software/theano/library/tensor/basic.html#constructors-with-optional-dtype).\n",
    "- Shared variables must be initialized with values of type `float32` in order to be stored on the GPU. This means that when you create a shared variable from a numpy array you must initialize the array with the `dtype=float32` argument, or cast the value using the `astype` function. See the [numpy data type documentation](http://docs.scipy.org/doc/numpy/user/basics.types.html) for more details.\n",
    "- Be very careful with data transfers to and from the GPU. Ideally you want all your data on the GPU. Ways to achieve this include initializing commonly uses data as shared variables with a `float32` data type, and to [avoid copying return values](http://deeplearning.net/software/theano/tutorial/using_gpu.html#returning-a-handle-to-device-allocated-data) back to the host using the `gpu_from_host` function. Theano includes [profiling capabilities](http://deeplearning.net/software/theano/tutorial/profiling.html) that allow you to debug how much time you are spending on transfers.\n",
    "\n",
    "With all that in mind, let's modify our code to run well on a GPU. I'll highlight the most important changes we made. The full code is available [here](https://github.com/dennybritz/nn-theano)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "\n",
    "# Use float32 as the default float data type\n",
    "theano.config.floatX = 'float32'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to get significant speedup from a GPU we should increase the size of our data. In that case the time will be dominated by our matrix multiplication and we get the most out of parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, train_y = sklearn.datasets.make_moons(5000, noise=0.20)\n",
    "nn_hdim = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To force the storage of our data on the GPU we use shared variables of type `float32`. Because we are constantly using the input data `X` and `y` to perform gradient updates we store those on the GPU as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These float32 values are initialized on the GPU\n",
    "X = theano.shared(train_X.astype('float32'), name='X')\n",
    "y = theano.shared(train_y_onehot.astype('float32'), name='y')\n",
    "W1 = theano.shared(np.random.randn(nn_input_dim, nn_hdim).astype('float32'), name='W1')\n",
    "b1 = theano.shared(np.zeros(nn_hdim).astype('float32'), name='b1')\n",
    "W2 = theano.shared(np.random.randn(nn_hdim, nn_output_dim).astype('float32'), name='W2')\n",
    "b2 = theano.shared(np.zeros(nn_output_dim).astype('float32'), name='b2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also modify our functions and remove inputs since the X and y variables are now  stored as shared variables on the GPU. We don't need to pass them to the functions anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradient_step = theano.function(\n",
    "    inputs=[],\n",
    "    updates=((W2, W2 - epsilon * dW2),\n",
    "             (W1, W1 - epsilon * dW1),\n",
    "             (b2, b2 - epsilon * db2),\n",
    "             (b1, b1 - epsilon * db1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "I tesed the code on a `g2.2xlarge` AWS EC2 instance. Running one `gradient_step()` on the CPU took around 250ms. Running it on the GPU with `THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32` took 7.5ms. That's a 30x speedup, and if our dataset or parameter space was larger we probably would have seen an even more significant speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 14.01 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "10000 loops, best of 3: 113 Âµs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit gradient_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full GPU-optimzied code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a dataset\n",
    "np.random.seed(0)\n",
    "train_X, train_y = sklearn.datasets.make_moons(5000, noise=0.20)\n",
    "train_y_onehot = np.eye(2)[train_y]\n",
    "\n",
    "# Size definitions\n",
    "num_examples = len(train_X) # training set size\n",
    "nn_input_dim = 2 # input layer dimensionality\n",
    "nn_output_dim = 2 # output layer dimensionality\n",
    "nn_hdim = 1000 # hiden layer dimensionality\n",
    "\n",
    "# Gradient descent parameters (I picked these by hand)\n",
    "epsilon = np.float32(0.01) # learning rate for gradient descent\n",
    "reg_lambda = np.float32(0.01) # regularization strength \n",
    "\n",
    "# GPU NOTE: Conversion to float32 to store them on the GPU!\n",
    "X = theano.shared(train_X.astype('float32')) # initialized on the GPU\n",
    "y = theano.shared(train_y_onehot.astype('float32'))\n",
    "\n",
    "# GPU NOTE: Conversion to float32 to store them on the GPU!\n",
    "W1 = theano.shared(np.random.randn(nn_input_dim, nn_hdim).astype('float32'), name='W1')\n",
    "b1 = theano.shared(np.zeros(nn_hdim).astype('float32'), name='b1')\n",
    "W2 = theano.shared(np.random.randn(nn_hdim, nn_output_dim).astype('float32'), name='W2')\n",
    "b2 = theano.shared(np.zeros(nn_output_dim).astype('float32'), name='b2')\n",
    "\n",
    "# Forward propagation\n",
    "z1 = X.dot(W1) + b1\n",
    "a1 = T.tanh(z1)\n",
    "z2 = a1.dot(W2) + b2\n",
    "y_hat = T.nnet.softmax(z2)\n",
    "\n",
    "# The regularization term (optional)\n",
    "loss_reg = 1./num_examples * reg_lambda/2 * (T.sum(T.sqr(W1)) + T.sum(T.sqr(W2))) \n",
    "# the loss function we want to optimize\n",
    "loss = T.nnet.categorical_crossentropy(y_hat, y).mean() + loss_reg\n",
    "# Returns a class prediction\n",
    "prediction = T.argmax(y_hat, axis=1)\n",
    "\n",
    "# Gradients\n",
    "dW2 = T.grad(loss, W2)\n",
    "db2 = T.grad(loss, b2)\n",
    "dW1 = T.grad(loss, W1)\n",
    "db1 = T.grad(loss, b1)\n",
    "\n",
    "# Note that we removed the input values because we will always use the same shared variable\n",
    "# GPU NOTE: Removed the input values to avoid copying data to the GPU.\n",
    "forward_prop = theano.function([], y_hat)\n",
    "calculate_loss = theano.function([], loss)\n",
    "predict = theano.function([], prediction)\n",
    "\n",
    "# GPU NOTE: Removed the input values to avoid copying data to the GPU.\n",
    "gradient_step = theano.function(\n",
    "    [],\n",
    "    # profile=True,\n",
    "    updates=((W2, W2 - epsilon * dW2),\n",
    "             (W1, W1 - epsilon * dW1),\n",
    "             (b2, b2 - epsilon * db2),\n",
    "             (b1, b1 - epsilon * db1)))\n",
    "\n",
    "def build_model(num_passes=20000, print_loss=False):\n",
    "    # Re-Initialize the parameters to random values. We need to learn these.\n",
    "    np.random.seed(0)\n",
    "    # GPU NOTE: Conversion to float32 to store them on the GPU!\n",
    "    W1.set_value((np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)).astype('float32'))\n",
    "    b1.set_value(np.zeros(nn_hdim).astype('float32'))\n",
    "    W2.set_value((np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)).astype('float32'))\n",
    "    b2.set_value(np.zeros(nn_output_dim).astype('float32'))\n",
    "    \n",
    "    # Gradient descent. For each batch...\n",
    "    for i in xrange(0, num_passes):\n",
    "        # This will update our parameters W2, b2, W1 and b1!\n",
    "        gradient_step()\n",
    "        \n",
    "        # Optionally print the loss.\n",
    "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            print \"Loss after iteration %i: %f\" %(i, calculate_loss())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Profiling\n",
    "# theano.config.profile = True\n",
    "# theano.config.profile_memory = True\n",
    "# gradient_step()\n",
    "# theano.printing.debugprint(gradient_step) \n",
    "# print gradient_step.profile.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 184 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit gradient_step()\n",
    "\n",
    "# start = time.time()\n",
    "# build_model()\n",
    "# print \"Total training time:\", time.time()-start, \"sec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
